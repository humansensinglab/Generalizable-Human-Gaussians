
<DOCTYPE html>

    <title>Generalizable Human Gaussians for Sparse View Synthesis</title>




    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-21408087-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-21408087-2');
    </script>

    <meta charset="utf-8">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="./static/css/style.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">

    <style>
        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fCBc4EsA.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBxc4EsA.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fChc4EsA.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBBc4.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7mxKOzY.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4WxKOzY.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7GxKOzY.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4mxK.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }
    </style>

    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <body>
        <div class="container">
            <div class="row mb-2 mt-4" id="paper-title">
                <h1 class="col-md-12 text-center">
                    Generalizable Human Gaussians (GHG)
                </h1>
                <h3 class="col-md-12 text-center">
                    for Sparse View Synthesis
                </h3>
                <h3 class="col-md-12 text-center">
                    <small>ECCV 2024</small>
                </h3>
            </div>

            <div class="row" id="authors">
                <div class="mx-auto text-center">
                    <ul class="list-inline mb-0">
                        <li class="list-inline-item">
                            <a href="https://youngjoongunc.github.io/">Youngjoong Kwon</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://github.com/baolef">Baole Fang&#42;</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://lewis-lv0.github.io/">Yixing Lu&#42;</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://www.haoyed.com/">Haoye Dong</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://czhang0528.github.io/">Cheng Zhang</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://scholar.google.com/citations?user=3elKp9wAAAAJ&hl=en">Francisco Vicente Carrasco</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://www.albertmosellamontoro.com/">Albert Mosella-Montoro</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://atlantixjj.github.io/">Jianjin Xu</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a>Shingo Takagi</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a>Daeil Kim</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a href="https://aayushp.github.io/">Aayush Prakash</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a href="https://www.cs.cmu.edu/~ftorre/">Fernando de la Torre</a><sup>1</sup>
                    </ul>
                    <p id="institution">
                        <sup>1</sup>Carnegie Mellon University &nbsp; &nbsp;
                        <sup>2</sup>Meta Reality Labs &nbsp; &nbsp;
                    </p>
                    <p id="equal-contribution">
                    &#42; Equal Contribution
                    </p>
                </div>
            </div>
            <div class="row mb-2" id="links">
                <div class="mx-auto">
                    <ul class="nav">
                        <li class="nav-item text-center">
                            <a href="http://arxiv.org/abs/2304.04897" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                                </svg><br>
                                Paper
                            </a>
                        <li class="nav-item text-center">
                            <a href="https://github.com/YoungJoongUNC/Neural_Image_Based_Avatars" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z" />
                                </svg><br>
                                Code
                            </a>
                        <li class="nav-item text-center">
                            <a href="https://drive.google.com/file/d/1tOb28TXhRdX3aiKnauuF6oyN2PinnGR8/view?usp=share_link" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M12,3C7.58,3 4,4.79 4,7C4,9.21 7.58,11 12,11C16.42,11 20,9.21 20,7C20,4.79 16.42,3 12,3M4,9V12C4,14.21 7.58,16 12,16C16.42,16 20,14.21 20,12V9C20,11.21 16.42,13 12,13C7.58,13 4,11.21 4,9M4,14V17C4,19.21 7.58,21 12,21C16.42,21 20,19.21 20,17V14C20,16.21 16.42,18 12,18C7.58,18 4,16.21 4,14Z" />
                                </svg><br>
                                Results
                            </a>
                    </ul>
                </div>
            </div>
            <!--<div class="row mb-3 pt-2">-->
            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">

                    <div id="dynamic-teaser">

                        <h6 style="color:#8899a5"> Generalizable Human Gaussians (GHG) can synthesize high-quality novel view renderings of arbitrary human subject from sparse multi-view images.</h6>
                        <div class="col-12 text-center">

                            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                                <source src="assets/videos/teaser_video2.mp4" type="video/mp4">
                            </video>
                        </div>
                        <!--<p class="text-justify">
                            In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific radiance fields to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects at test time. Adopting such approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates trackable visual features based on the skeletal body motions over video frames. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses.
                        </p>-->
                    </div> <!-- dynamic-teaser -->
                </div>
            </div>

            <div class="row mb-3">
                <div class="col-md-8 mx-auto">
                    <h4 class="pb-2">Abstract</h4>
                    <p class="text-justify">
                        Recent progress in neural rendering have brought forth pioneering methods, such as NeRF and Gaussian Splatting, which revolutionize view rendering across various domains like AR/VR, gaming, and content creation.
                        While these methods excel at interpolating within the training data, the challenge of generalizing to new scenes and objects from very sparse views persists.
                        Specifically, modeling 3D humans from sparse views presents formidable hurdles due to the inherent complexity of human geometry, resulting in inaccurate reconstructions of geometry and textures.
                        To tackle this challenge, this paper leverages recent advancements in Gaussian splatting and introduces a new method to learn generalizable human Gaussians that allows photorealistic and accurate view-rendering of a new human subject from a limited set of sparse views in a feed-forward manner.
                        A pivotal innovation of our approach involves reformulating the learning of 3D Gaussian parameters into a regression process defined on the 2D UV space of a human template, which allows leveraging the strong geometry prior and the advantages of 2D convolutions.
                        Our method outperforms recent methods on both within-dataset generalization as well as cross-dataset generalization settings.
                    </p>

                </div>
            </div>


            <div class="row mb-4" id="overview">
                <div class="col-md-8 mx-auto grey-container">
                    <h4 class="pb-2">Overview</h4>
                    <p class="text-justify">
                    (a) We focus on generalizable human rendering under very sparse view setting.
                        (b) We first construct the multi-scaffolds by dilating the human template surface.
                        The 2D UV space of each scaffold serves to collect the geometry and appearance information from the corresponding 3D locations.
                        (c) The aggregated multi-scaffold input is fed into the network, which generates multi-Gaussian parameter maps.
                        (d) Finally, Gaussians are anchored on the corresponding surface of each scaffold, and rasterized into novel views.
                    </p>
                    <img src="./assets/images/overview_png.png" class="img-responsive" alt="pipeline">
                </div>
            </div>

    <div class="row mb-3 pt-2">
    <div class="col-md-8 mx-auto">

    <div id="in-domain">
        <h4>In-domain Generalization - Trained and Tested on THuman 2.0</h4>
                        <p class="text-justify">
                            Our method utilizes a 3D human prior to achieve robust and multi-view consistent novel view renderings under very sparse view setting.
                            In addition, our method collects visual information from the multi-scaffold representations and thus recovers sharp and high-frequency details including hair, wrinkles, and logos.
                        </p>
    <div class="slider-container" id="slider1">
        <div class="slider">
            <video class="video-slide" src="./assets/videos/page_2.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
            <video class="video-slide" src="./assets/videos/page_3.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
            <video class="video-slide" src="./assets/videos/page_4.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
            <video class="video-slide" src="./assets/videos/page_5.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
            <video class="video-slide" src="./assets/videos/page_6.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
            <video class="video-slide" src="./assets/videos/page_7.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
            <video class="video-slide" src="./assets/videos/page_8.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
        </div>
        <button class="prev" onclick="changeSlide(-1, 'slider1')">&#10094;</button>
        <button class="next" onclick="changeSlide(1, 'slider1')">&#10095;</button>
    </div>

    <!--<script src="./static/js/script.js"></script>-->
    </div>
    </div>
    </div>


    <div class="row mb-3 pt-2">
    <div class="col-md-8 mx-auto">

    <div id="cross-domain">
        <h4>Cross-domain Generalization - Trained on THuman 2.0 Tested on RenderPeople</h4>
                        <p class="text-justify">
                            We achieve high-quality synthesis under the challenging cross-domain generalization setting.
                        </p>
    <div class="slider-container">
        <div class="slider" id="slider2">
            <video class="video-slide" src="./assets/videos/page_9.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
            <video class="video-slide" src="./assets/videos/page_10.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
            <video class="video-slide" src="./assets/videos/page_11.mp4" controls width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted=""></video>
        </div>
        <button class="prev" onclick="changeSlide(-1, 'slider2')">&#10094;</button>
        <button class="next" onclick="changeSlide(1, 'slider2')">&#10095;</button>
    </div>


    </div>
    </div>
    </div>
    <script src="./static/js/script.js"></script>

            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">

                    <div id="multi-scaffold">
                        <h4>Effectiveness of Multi-Scaffold</h4>
                        <p class="text-justify">
                            Each column shows different scaffold levels, with the last column illustrating their combined effect.
                            The top part shows the RGB representation, while the bottom part highlights affected regions, with grey indicating unaffected areas.
                        </p>

                        <div class="col-12 text-center">

                            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                                <source src="./assets/videos/scaffold_visualization_video_1s.mp4" type="video/mp4">
                            </video>
                        </div>

                    </div> <!-- dynamic-teaser -->
                </div>
            </div>



            <div class="row mb-4" id="main-video">
                <div class="col-md-8 mx-auto grey-container">
                    <h4 class="pb-2">Main Video</h4>
                    <p>
                        In this video, in-domain and cross-domain generalization results are shown.
                        Note that GPS-Gaussian* is trained and tested with 5 input views due to the rectification requirement,
                        whereas NHP, NIA and our method are trained and tested with 3 input views.
                    </p>>
                    <div class="embed-responsive embed-responsive-16by9">
                        <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/PyST3-dfmfI?si=li5r8mCGYZNbfYNs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </div>
                </div>
            </div>









            <!--<div class="row mb-3">
                <div class="col-md-8 mx-auto">
                    <h4>Related Links</h4>
                    <ul>
                        <li>
                            NeRF was introduced in <a href="https://www.matthewtancik.com/nerf">Mildenhall et al. (2020)</a>
                        <li>
                            Local image features were used in the related regime of implicit surfaces in
                            <a href="https://shunsukesaito.github.io/PIFu/">Saito et al. (2019)</a>
                            and
                            <a href="https://arxiv.org/abs/1905.10711">Xu et al. (2019)</a>
                        <li>
                            Our MLP architecture is
                            inspired by
                            <a href="https://avg.is.tuebingen.mpg.de/publications/niemeyer2020cvpr">DVR</a>
                        <li>
                            Parts of our
                            PyTorch NeRF implementation are taken from
                            <a href="https://github.com/kwea123/nerf_pl">kwea123</a>
                        <li>
                            Also see the concurrent work
                            <a href="https://arxiv.org/abs/2010.04595">GRF</a>
                            which also introduces image features for NeRF, showing image features can even improve NeRF when a large number of views are available.
                    </ul>
                </div>
            </div> -->
            <!--<div class="row mb-4">
                <div class="col-md-8 mx-auto">
                    <h4 class="mb-3">Bibtex</h4>
                    <textarea id="bibtex" class="form-control" readonly>@inproceedings{yu2020pixelnerf,
      title={{pixelNeRF}: Neural Radiance Fields from One or Few Images},
      author={Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
      year={2021},
      booktitle={CVPR},
}</textarea>
                </div>
            </div>-->
            <!--<div class="row mb-3">
                <div class="col-md-8 mx-auto">
                    <h4>Acknowledgements</h4>
                    <p class="text-justify">
                        We thank <a href="https://pengsida.net/">Sida Peng</a> of Zhejiang University for very many helpful discussions on a variety of implementation details of <a href="https://github.com/zju3dv/animatable_nerf">Animatable-NeRF</a>. We thank <a href="https://lemonatsu.github.io/">Shih-Yang Su</a> of University of British Columbia for helpful discussions on the <a href="https://lemonatsu.github.io/anerf/">A-NeRF</a> details. We thank <a href="https://www.cs.ubc.ca/~rhodin/web/">Prof. Helge Rhodin and his group</a> for the insightful discussions on the human performance capture. This work was partially supported by National Science Foundation Award 2107454.
                        We also thank <a href="https://alexyu.net/">Alex Yu</a> for the template of this website.
                    </p>
                </div>
            </div>-->
        </div> <!-- container -->
    </body>